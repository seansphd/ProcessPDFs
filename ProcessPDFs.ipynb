{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/UQ7lP9FgyCTXfM5/Tat7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seansphd/ProcessPDFs/blob/main/ProcessPDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_WP3TGxP55h9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "d9220be5-1f7c-473f-c93c-4c294a9c42b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking and installing required packages...\n",
            "Installing pdf2image...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1277791689.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_packages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"PyPDF2\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"PyPDF2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minstall_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0m_import_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0mimport_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_import_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_import_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mdefine_import_structure\u001b[0;34m(module_path, prefix)\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwill\u001b[0m \u001b[0madd\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0mto\u001b[0m \u001b[0mall\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \"\"\"\n\u001b[0;32m-> 2851\u001b[0;31m     \u001b[0mimport_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m     \u001b[0mspread_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspread_import_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2562\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__pycache__\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m             \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2563\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__pycache__\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2564\u001b[0m             \u001b[0mimport_structure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_import_structure_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/genericpath.py\u001b[0m in \u001b[0;36misdir\u001b[0;34m(s)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# PDF OCR and summarisation with batch processing for GitHub repositories\n",
        "# Output format: [{\"filename\": \"...\",\"summary\":\"...\",\"url\":\"...\"}]\n",
        "\n",
        "# ======== INSTALL DEPENDENCIES ========\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_package(package):\n",
        "    print(f\"Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "required_packages = [\n",
        "    \"pytesseract\",\n",
        "    \"pdf2image\",\n",
        "    \"transformers\",\n",
        "    \"torch\",\n",
        "    \"sentencepiece\",\n",
        "    \"einops\",\n",
        "    \"accelerate\",\n",
        "    \"spacy\",\n",
        "    \"textstat\",\n",
        "    \"tqdm\",\n",
        "    \"numpy\",\n",
        "    \"requests\",\n",
        "    \"PyPDF2\",\n",
        "    \"Pillow\"\n",
        "]\n",
        "\n",
        "print(\"Checking and installing required packages...\")\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package if package != \"PyPDF2\" else \"PyPDF2\")\n",
        "    except ImportError:\n",
        "        install_package(package)\n",
        "\n",
        "# Install system dependencies if in Google Colab\n",
        "import os\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Installing system dependencies for Colab...\")\n",
        "    subprocess.run([\"apt-get\", \"update\"], check=True)\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"poppler-utils\", \"tesseract-ocr\", \"tesseract-ocr-eng\"], check=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "\n",
        "# ======== IMPORTS ========\n",
        "# Core libraries\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import tempfile\n",
        "import requests\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "\n",
        "# PDF processing\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import PyPDF2  # For direct text extraction from PDFs\n",
        "\n",
        "# NLP and Text Analysis\n",
        "import spacy\n",
        "import textstat\n",
        "\n",
        "# ML and Summarisation\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# GitHub API\n",
        "from urllib.parse import urljoin\n",
        "import urllib.parse\n",
        "\n",
        "# ======== SETUP ========\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load spaCy for NER and text analysis\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define available models\n",
        "available_models = {\n",
        "    \"BART CNN (fast)\": \"facebook/bart-large-cnn\",\n",
        "    \"T5 Small (fastest)\": \"t5-small\",\n",
        "    \"T5 Base (balanced)\": \"t5-base\",\n",
        "    \"FLAN-T5 Base (recommended)\": \"google/flan-t5-base\",\n",
        "    \"FLAN-T5 Large (better quality)\": \"google/flan-t5-large\"\n",
        "}\n",
        "\n",
        "# ======== GITHUB API FUNCTIONS ========\n",
        "def list_github_pdf_files(repo_owner, repo_name, path=\"\", token=None):\n",
        "    \"\"\"\n",
        "    List all PDF files in a GitHub repository recursively.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: {name, path, download_url, html_url}\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github+json\",\n",
        "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
        "    }\n",
        "    if token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "\n",
        "    pdf_files = []\n",
        "\n",
        "    def fetch_contents(path_segment):\n",
        "        if path_segment:\n",
        "            url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{path_segment}\"\n",
        "        else:\n",
        "            url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents\"\n",
        "\n",
        "        print(f\"Accessing URL: {url}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=30)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error fetching contents: {response.status_code}\")\n",
        "                error_message = \"Unknown error\"\n",
        "                try:\n",
        "                    error_data = response.json()\n",
        "                    error_message = error_data.get('message', 'Unknown error')\n",
        "                except Exception:\n",
        "                    pass\n",
        "                print(error_message)\n",
        "\n",
        "                if response.status_code == 401 and token and \"Bearer\" in headers.get(\"Authorization\", \"\"):\n",
        "                    print(\"Trying alternative authorisation format...\")\n",
        "                    headers[\"Authorization\"] = f\"token {token}\"\n",
        "                    retry_response = requests.get(url, headers=headers, timeout=30)\n",
        "                    if retry_response.status_code == 200:\n",
        "                        print(\"Alternative authorisation successful\")\n",
        "                        response = retry_response\n",
        "                    else:\n",
        "                        print(f\"Alternative authorisation also failed: {retry_response.status_code}\")\n",
        "                        return\n",
        "                else:\n",
        "                    return\n",
        "\n",
        "            contents = response.json()\n",
        "\n",
        "            if isinstance(contents, list):\n",
        "                for item in contents:\n",
        "                    if item[\"type\"] == \"file\" and item[\"name\"].lower().endswith(\".pdf\"):\n",
        "                        pdf_files.append({\n",
        "                            \"name\": item[\"name\"],\n",
        "                            \"path\": item[\"path\"],\n",
        "                            \"download_url\": item[\"download_url\"],\n",
        "                            \"html_url\": item.get(\"html_url\")\n",
        "                        })\n",
        "                    elif item[\"type\"] == \"dir\":\n",
        "                        fetch_contents(item[\"path\"])\n",
        "            elif isinstance(contents, dict) and contents.get(\"type\") == \"file\" and contents[\"name\"].lower().endswith(\".pdf\"):\n",
        "                pdf_files.append({\n",
        "                    \"name\": contents[\"name\"],\n",
        "                    \"path\": contents[\"path\"],\n",
        "                    \"download_url\": contents[\"download_url\"],\n",
        "                    \"html_url\": contents.get(\"html_url\")\n",
        "                })\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request error: {e}\")\n",
        "\n",
        "    fetch_contents(path)\n",
        "    return pdf_files\n",
        "\n",
        "# ======== PDF PROCESSING FUNCTIONS ========\n",
        "def download_pdf(url, output_path):\n",
        "    \"\"\"Download a PDF file from a URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(output_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    return output_path\n",
        "\n",
        "def ocr_pdf(pdf_path, dpi=300, preprocess=True, lang='eng'):\n",
        "    \"\"\"Extract text from PDF using OCR with optional image preprocessing.\"\"\"\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=dpi)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF to images: {e}\")\n",
        "        return f\"Error processing PDF: {e}\", []\n",
        "\n",
        "    full_text = \"\"\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        if preprocess:\n",
        "            img_np = np.array(image)\n",
        "            img = Image.fromarray(img_np)\n",
        "            img = img.convert('L')  # greyscale\n",
        "            img = ImageEnhance.Contrast(img).enhance(2.0)\n",
        "            img = img.filter(ImageFilter.SHARPEN)\n",
        "            threshold = 150\n",
        "            img = img.point(lambda p: 255 if p > threshold else 0)\n",
        "            text = pytesseract.image_to_string(img, lang=lang, config='--psm 6')\n",
        "        else:\n",
        "            text = pytesseract.image_to_string(image, lang=lang)\n",
        "\n",
        "        full_text += f\"\\n\\n--- Page {i+1} ---\\n\\n{text}\"\n",
        "\n",
        "    return full_text, images\n",
        "\n",
        "def extract_text_directly(pdf_path):\n",
        "    \"\"\"Extract text directly from PDF without OCR.\"\"\"\n",
        "    try:\n",
        "        full_text = \"\"\n",
        "        num_pages = 0\n",
        "\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "\n",
        "            for i, page in enumerate(pdf_reader.pages):\n",
        "                text = page.extract_text() or \"\"\n",
        "                full_text += f\"\\n\\n--- Page {i+1} ---\\n\\n{text}\"\n",
        "\n",
        "        images = [None] * num_pages\n",
        "        return full_text, images\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text directly from PDF: {e}\")\n",
        "        return f\"Error processing PDF: {e}\", []\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Light cleanup after extraction.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Collapse whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Keep Unicode. Avoid heavy replacements that alter content.\n",
        "    # Fix common OCR punctuation spacing\n",
        "    text = re.sub(r'([.,;:!?])(\\w)', r'\\1 \\2', text)\n",
        "    return text.strip()\n",
        "\n",
        "# ======== TEXT ANALYSIS FUNCTIONS ========\n",
        "def summarise_text(text, model_name=\"facebook/bart-large-cnn\", chunk_size=1024, max_length=150, min_length=50):\n",
        "    \"\"\"Summarise text with a Hugging Face model.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text to summarise.\"\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "        summariser = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
        "\n",
        "        prefix = \"summarize: \" if \"t5\" in model_name else \"\"\n",
        "\n",
        "        # Sentence aware chunking using spaCy\n",
        "        doc = nlp(text)\n",
        "        sentences = [s.text for s in doc.sents]\n",
        "\n",
        "        chunks = []\n",
        "        buf = \"\"\n",
        "        for s in sentences:\n",
        "            if len(tokenizer.encode(buf + s)) < chunk_size:\n",
        "                buf += s + \" \"\n",
        "            else:\n",
        "                if buf.strip():\n",
        "                    chunks.append(buf.strip())\n",
        "                buf = s + \" \"\n",
        "        if buf.strip():\n",
        "            chunks.append(buf.strip())\n",
        "\n",
        "        summaries = []\n",
        "        for chunk in chunks:\n",
        "            if not chunk.strip():\n",
        "                continue\n",
        "            input_text = prefix + chunk\n",
        "            try:\n",
        "                out = summariser(input_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "                summaries.append(out[0]['summary_text'])\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarising chunk: {e}\")\n",
        "                continue\n",
        "\n",
        "        combined = \" \".join(summaries).strip()\n",
        "\n",
        "        if not combined:\n",
        "            combined = text[:800]\n",
        "\n",
        "        # Optional second pass if the combined text is still very long\n",
        "        if len(tokenizer.encode(combined)) > chunk_size:\n",
        "            combined = summariser(prefix + combined, max_length=max_length*2, min_length=min_length, do_sample=False)[0]['summary_text']\n",
        "\n",
        "        return combined\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarisation: {e}\")\n",
        "        return f\"Error in summarisation: {e}\"\n",
        "\n",
        "def extract_keywords(text, top_n=10):\n",
        "    \"\"\"Extract keywords from text.\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text[:500000])  # safety limit\n",
        "        keywords = []\n",
        "\n",
        "        for chunk in doc.noun_chunks:\n",
        "            keywords.append(chunk.text)\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            keywords.append(ent.text)\n",
        "\n",
        "        keyword_counter = Counter(keywords)\n",
        "\n",
        "        filtered = {}\n",
        "        for k, v in keyword_counter.items():\n",
        "            tk = nlp(k)\n",
        "            if len(k) > 3 and not all(t.is_stop for t in tk):\n",
        "                filtered[k] = v\n",
        "\n",
        "        top_keywords = dict(sorted(filtered.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
        "        return top_keywords\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def analyse_text(text):\n",
        "    \"\"\"Compute text statistics.\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        stats = {\n",
        "            'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
        "            'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),\n",
        "            'smog_index': textstat.smog_index(text),\n",
        "            'coleman_liau_index': textstat.coleman_liau_index(text),\n",
        "            'automated_readability_index': textstat.automated_readability_index(text),\n",
        "            'dale_chall_readability_score': textstat.dale_chall_readability_score(text),\n",
        "            'word_count': textstat.lexicon_count(text),\n",
        "            'sentence_count': textstat.sentence_count(text),\n",
        "            'avg_sentence_length': textstat.avg_sentence_length(text),\n",
        "            'avg_syllables_per_word': textstat.avg_syllables_per_word(text)\n",
        "        }\n",
        "        return stats\n",
        "    except Exception as e:\n",
        "        print(f\"Error analysing text: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"Named Entity Recognition.\"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text[:500000])  # safety limit\n",
        "        entities = {}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ not in entities:\n",
        "                entities[ent.label_] = []\n",
        "            if ent.text not in entities[ent.label_]:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "\n",
        "        return entities\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting entities: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# ======== MAIN PROCESSING FUNCTION ========\n",
        "def process_pdf(url, pdf_name, model_name=\"facebook/bart-large-cnn\", dpi=300, preprocess=True, use_ocr=True):\n",
        "    \"\"\"Process a single PDF file and return summary text.\"\"\"\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            pdf_path = os.path.join(temp_dir, \"document.pdf\")\n",
        "            download_pdf(url, pdf_path)\n",
        "\n",
        "            if use_ocr:\n",
        "                print(f\"Performing OCR on {pdf_name}...\")\n",
        "                raw_text, images = ocr_pdf(pdf_path, dpi=dpi, preprocess=preprocess)\n",
        "            else:\n",
        "                print(f\"Extracting text directly from {pdf_name}...\")\n",
        "                raw_text, images = extract_text_directly(pdf_path)\n",
        "\n",
        "            text = clean_text(raw_text)\n",
        "            summary_text = summarise_text(text, model_name=model_name)\n",
        "            return summary_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_name}: {e}\")\n",
        "        return f\"Error processing file: {e}\"\n",
        "\n",
        "# ======== JSON SAVE FUNCTION ========\n",
        "def save_results_to_json(results, output_file):\n",
        "    \"\"\"Save processing results to a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results to JSON: {e}\")\n",
        "\n",
        "# ======== RECORD FORMATTER ========\n",
        "def format_simple_record(pdf_file, summary_text):\n",
        "    \"\"\"\n",
        "    Create the minimal record:\n",
        "    {\n",
        "      \"filename\": \"<name>\",\n",
        "      \"summary\": \"<summary_text>\",\n",
        "      \"url\": \"<html_url or download_url>\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"filename\": pdf_file[\"name\"],\n",
        "        \"summary\": summary_text,\n",
        "        \"url\": pdf_file.get(\"html_url\") or pdf_file.get(\"download_url\")\n",
        "    }\n",
        "\n",
        "# ======== BATCH PROCESSING FUNCTION ========\n",
        "def batch_process_pdfs(pdf_files, model_name=\"facebook/bart-large-cnn\", dpi=300,\n",
        "                       preprocess=True, use_ocr=True, output_json=\"pdf_summaries.json\"):\n",
        "    \"\"\"\n",
        "    Process multiple PDF files and save results to JSON as a flat array of:\n",
        "    { \"filename\": ..., \"summary\": ..., \"url\": ... }\n",
        "    \"\"\"\n",
        "    simple_results = []\n",
        "\n",
        "    for i, pdf_file in enumerate(tqdm(pdf_files, desc=\"Processing PDFs\")):\n",
        "        print(f\"\\n[{i+1}/{len(pdf_files)}] Processing: {pdf_file['name']}\")\n",
        "\n",
        "        # Run the summarisation pipeline\n",
        "        summary_text = process_pdf(\n",
        "            pdf_file[\"download_url\"],\n",
        "            pdf_file[\"name\"],\n",
        "            model_name=model_name,\n",
        "            dpi=dpi,\n",
        "            preprocess=preprocess,\n",
        "            use_ocr=use_ocr\n",
        "        )\n",
        "\n",
        "        record = format_simple_record(pdf_file, summary_text)\n",
        "        simple_results.append(record)\n",
        "\n",
        "        # Save progress after each file\n",
        "        save_results_to_json(simple_results, output_json)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return simple_results\n",
        "\n",
        "# ======== MAIN FUNCTION ========\n",
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    # Configuration\n",
        "    repo_owner = input(\"Enter GitHub repository owner: \")\n",
        "    repo_name = input(\"Enter GitHub repository name: \")\n",
        "    path = input(\"Enter repository path (leave blank for root): \")\n",
        "    token = input(\"Enter GitHub token (leave blank if not needed): \")\n",
        "\n",
        "    if not token:\n",
        "        token = None\n",
        "\n",
        "    use_ocr = input(\"\\nDo the PDFs require OCR? (y/n): \").lower() == 'y'\n",
        "\n",
        "    dpi = 300\n",
        "    preprocess = True\n",
        "    if use_ocr:\n",
        "        try:\n",
        "            dpi = int(input(\"Enter OCR resolution (DPI, recommended 300): \") or \"300\")\n",
        "        except ValueError:\n",
        "            dpi = 300\n",
        "        preprocess = input(\"Enhance images before OCR? (y/n): \").lower() == 'y'\n",
        "    else:\n",
        "        print(\"Skipping OCR and using direct text extraction...\")\n",
        "\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, (name, _) in enumerate(available_models.items()):\n",
        "        print(f\"{i+1}. {name}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            model_idx = int(input(\"\\nSelect model (1-5): \")) - 1\n",
        "            if 0 <= model_idx < len(available_models):\n",
        "                break\n",
        "        except ValueError:\n",
        "            pass\n",
        "        print(\"Please enter a number between 1 and 5.\")\n",
        "    model_name = list(available_models.values())[model_idx]\n",
        "\n",
        "    output_json = input(\"Enter output JSON filename (default: pdf_summaries.json): \") or \"pdf_summaries.json\"\n",
        "\n",
        "    print(\"\\nFetching PDF files from repository...\")\n",
        "    pdf_files = list_github_pdf_files(repo_owner, repo_name, path, token)\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"No PDF files found in the repository.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(pdf_files)} PDF files:\")\n",
        "    for i, pdf in enumerate(pdf_files):\n",
        "        print(f\"{i+1}. {pdf['path']}\")\n",
        "\n",
        "    confirm = input(f\"\\nProcess {len(pdf_files)} PDFs? (y/n): \").lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"Operation cancelled.\")\n",
        "        return\n",
        "\n",
        "    results = batch_process_pdfs(\n",
        "        pdf_files,\n",
        "        model_name=model_name,\n",
        "        dpi=dpi,\n",
        "        preprocess=preprocess,\n",
        "        use_ocr=use_ocr,\n",
        "        output_json=output_json\n",
        "    )\n",
        "\n",
        "    print(f\"\\nProcessing complete. Results saved to {output_json}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}